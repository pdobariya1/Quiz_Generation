Most modern deep learning models are based on multi-layered neural networks such as 
convolutional neural networks and transformers, although they can also include propositional 
formulas or latent variables organized layer-wise in deep generative models such as the nodes in 
deep belief networks and deep Boltzmann machines.[7]

Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy 
of layers is used to transform input data into a slightly more abstract and composite 
representation. For example, in an image recognition model, the raw input may be an image 
(represented as a tensor of pixels). The first representational layer may attempt to identify basic 
shapes such as lines and circles, the second layer may compose and encode arrangements of edges, 
the third layer may encode a nose and eyes, and the fourth layer may recognize that the image 
contains a face.

Importantly, a deep learning process can learn which features to optimally place at which level on 
its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature 
engineering to transform the data into a more suitable representation for a classification 
algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model 
discovers useful feature representations from the data automatically. This does not eliminate the 
need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different 
degrees of abstraction.[8][2]

The word "deep" in "deep learning" refers to the number of layers through which the data is 
transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) 
depth. The CAP is the chain of transformations from input to output. CAPs describe potentially 
causal connections between input and output. For a feedforward neural network, the depth of the 
CAPs is that of the network and is the number of hidden layers plus one (as the output layer is 
also parameterized). For recurrent neural networks, in which a signal may propagate through a layer 
more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of 
depth divides shallow learning from deep learning, but most researchers agree that deep learning 
involves CAP depth higher than two.